from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re 

pages = set()

def getLinks(pageUrl):    
    try:
        html = urlopen('https://www.python.org/{}'.format(pageUrl))          
        bs = BeautifulSoup(html, 'html.parser')     
        for link in bs.find_all('a', href=re.compile('^(/downloads/)((?!:).)*$')):  # Looking for external links
            if 'href' in link.attrs:             
                if link.attrs['href'] not in pages:                 
                    # We have encountered a new page                 
                    newPage = link.attrs['href']                 
                    print(newPage)                 
                    pages.add(newPage)                 
                    getLinks(newPage) 
    except Exception as e:
        print(f"Error retrieving page: {e}")

getLinks('')
